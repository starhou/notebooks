{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Attention.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPylzSGU6cRUiy+w+RNbiCw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/starhou/notebooks/blob/master/ML/Attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WVkGrqp7w5i1"
      },
      "source": [
        "# SelfAttention 理解\n",
        "\n",
        "感谢\n",
        "**伟大是熬出来的** 大佬的分享[超详细图解Self-Attention](https://zhuanlan.zhihu.com/p/410776234)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Hb8gkPDxX2n"
      },
      "source": [
        "## 原始公式\n",
        "\n",
        "$$\n",
        "\\operatorname{Attention}(Q, K, V)=\\operatorname{softmax}\\left(\\frac{Q K^{T}}{\\sqrt{d_{k}}}\\right) V\n",
        "$$\n",
        "\n",
        "  ![](https://pic4.zhimg.com/80/v2-6b6030a342a43d7c220cdc940738b783_720w.jpg)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cdDvkyz-wIeV",
        "outputId": "53b6cb6b-8397-48fb-dad6-d93f9fce1328"
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "# TensorFlow and tf.keras\n",
        "import tensorflow as tf\n",
        "\n",
        "import os\n",
        "import io\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "\n",
        "\n",
        "# Helper libraries\n",
        "import imageio\n",
        "import datetime\n",
        "import numpy as np\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython import display\n",
        "import PIL\n",
        "import glob\n",
        "from scipy import signal\n",
        "print(tf.__version__)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O8upRPhSAIVF"
      },
      "source": [
        "class SelfAttention(tf.keras.Model):\n",
        "  def __init__(self, input_dim = 3, dim_q_k = 4, dim_v = 5):\n",
        "    super(SelfAttention, self).__init__()\n",
        "    self.input_size = input_dim\n",
        "    self.key_size = dim_q_k\n",
        "    self.value_size = dim_v\n",
        "    self.query_kernel = self.add_weight(\n",
        "      name=\"query_kernel\",\n",
        "      shape=[self.input_size, self.input_size, self.key_size],\n",
        "      trainable=True,\n",
        "    )\n",
        "    self.key_kernel = self.add_weight(\n",
        "        name=\"key_kernel\",\n",
        "        shape=[self.input_size, self.input_size, self.key_size],\n",
        "        trainable=True,\n",
        "    )\n",
        "    self.value_kernel = self.add_weight(\n",
        "        name=\"value_kernel\",\n",
        "        shape=[self.input_size, self.input_size, self.value_size],\n",
        "        trainable=True,\n",
        "    )\n",
        "\n",
        "  def call(self, x):\n",
        "    Q = tf.einsum('...b,bbc->...bc', x, self.query_kernel)\n",
        "    K = tf.einsum('...b,bbc->...bc', x, self.key_kernel)\n",
        "    V = tf.einsum('...b,bbc->...bc', x, self.value_kernel)\n",
        "    W = tf.einsum('...ad,...cd->...ac', Q, K)\n",
        "\n",
        "    out = tf.einsum('...ab,...bc->...ac', W, V)\n",
        "    return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pKVDoXb8Gxtb"
      },
      "source": [
        "selfAttention = SelfAttention()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pSt1MsuyG40V"
      },
      "source": [
        "x = tf.ones((10, 3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MlzDMx59HJZM"
      },
      "source": [
        "selfAttention(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eFsijfEZzut_"
      },
      "source": [
        "## MultiHeadAttention\n",
        "Scaled Dot-Product Attention 过程做 H 次，再把输出合并起来。\n",
        "\n",
        "![链接文字](https://pic3.zhimg.com/80/v2-f221c5a13a4e6e3fb84685e0f884b1da_720w.jpg)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qdGwsafs0H_6"
      },
      "source": [
        "class MultiHeadAttention(tf.keras.Model):\n",
        "  def __init__(self, head_size = 3, \n",
        "            num_heads = 4, \n",
        "            output_size = 5,\n",
        "            num_query_features = 6,\n",
        "            num_key_features = 6,\n",
        "            num_value_features = 6,\n",
        "            ):\n",
        "    super(MultiHeadAttention, self).__init__()\n",
        "    self.head_size = head_size\n",
        "    self.num_heads = num_heads\n",
        "    self.output_size = output_size\n",
        "    self.query_kernel = self.add_weight(\n",
        "        name=\"query_kernel\",\n",
        "        shape=[self.num_heads, num_query_features, self.head_size],\n",
        "    )\n",
        "    self.key_kernel = self.add_weight(\n",
        "        name=\"key_kernel\",\n",
        "        shape=[self.num_heads, num_key_features, self.head_size],\n",
        "    )\n",
        "    self.value_kernel = self.add_weight(\n",
        "        name=\"value_kernel\",\n",
        "        shape=[self.num_heads, num_value_features, self.head_size],\n",
        "    )\n",
        "    self.projection_kernel = self.add_weight(\n",
        "        name=\"projection_kernel\",\n",
        "        shape=[self.num_heads, self.head_size, output_size],\n",
        "    )\n",
        "    self.projection_bias = self.add_weight(\n",
        "      name=\"projection_bias\",\n",
        "      shape=[output_size],\n",
        "    )\n",
        "  def call(self, inputs):\n",
        "    # einsum nomenclature\n",
        "    # ------------------------\n",
        "    # N = query elements\n",
        "    # M = key/value elements\n",
        "    # H = heads\n",
        "    # I = input features\n",
        "    # O = output features\n",
        "    query = inputs[0]\n",
        "    key = inputs[1]\n",
        "    value = inputs[2] if len(inputs) > 2 else key\n",
        "    # Linear transformations\n",
        "    query = tf.einsum(\"...NI , HIO -> ...NHO\", query, self.query_kernel)\n",
        "    key = tf.einsum(\"...MI , HIO -> ...MHO\", key, self.key_kernel)\n",
        "    value = tf.einsum(\"...MI , HIO -> ...MHO\", value, self.value_kernel)\n",
        "    depth = tf.constant(self.head_size, dtype=query.dtype)\n",
        "    query /= tf.sqrt(depth)\n",
        "\n",
        "    # Calculate dot product attention\n",
        "    logits = tf.einsum(\"...NHO,...MHO->...HNM\", query, key)\n",
        "    attn_coef = tf.nn.softmax(logits)\n",
        "    # attention * value\n",
        "    multihead_output = tf.einsum(\"...HNM,...MHI->...NHI\", attn_coef, value)\n",
        "    output = tf.einsum(\n",
        "        \"...NHI,HIO->...NO\", multihead_output, self.projection_kernel\n",
        "    )\n",
        "    return output"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HVOQf5zAikC2"
      },
      "source": [
        "multiHeadAttention = MultiHeadAttention()"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aw7uhxckisry"
      },
      "source": [
        "x = tf.ones((10, 7, 20, 3, 6))"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "trTpZpAIjyIE"
      },
      "source": [
        "multiHeadAttention(x)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}