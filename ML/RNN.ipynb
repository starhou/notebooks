{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPmFP5FQ28U4eBrhhS8LJoX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/starhou/Algorithm/blob/master/ML/RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aklNDoJhAOBb",
        "colab_type": "text"
      },
      "source": [
        "# RNN ,LSTM and GRU理解及公式\n",
        "[参考文章](http://nicodjimenez.github.io/2014/08/08/lstm.html) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tcDmtHoNCd6_",
        "colab_type": "text"
      },
      "source": [
        "### LSTM\n",
        "\n",
        "\n",
        "![替代文字](https://upload.wikimedia.org/wikipedia/commons/thumb/3/3b/The_LSTM_cell.png/1280px-The_LSTM_cell.png)\n",
        "\n",
        "\n",
        "![遗忘门](https://images2015.cnblogs.com/blog/103496/201704/103496-20170409165115175-472920522.png)\n",
        "\n",
        ">$f(t)=\\sigma\\left(W_{f x} x(t)+W_{f h} h(t-1)+b_{f}\\right)$\n",
        "\n",
        "\n",
        "![](https://images2015.cnblogs.com/blog/103496/201704/103496-20170409170422988-352278153.png)\n",
        "\n",
        ">$g(t)=tanh\\left(W_{g x} x(t)+W_{g h} h(t-1)+b_{g}\\right)$ \n",
        "\n",
        ">$i(t)=\\sigma\\left(W_{i x} x(t)+W_{i h} h(t-1)+b_{i}\\right)$\n",
        "\n",
        "\n",
        "\n",
        "![](https://images2015.cnblogs.com/blog/103496/201704/103496-20170409170858269-146429766.png)\n",
        "\n",
        "\n",
        "> $s(t)=g(t) * i(t)+s(t-1) * f(t)$\n",
        "\n",
        "![](https://images2015.cnblogs.com/blog/103496/201704/103496-20170409171736644-170667020.png)\n",
        "\n",
        "> $o(t)=\\sigma\\left(W_{o x} x(t)+W_{o h} h(t-1)+b_{o}\\right)$\n",
        "\n",
        "> $h(t)=tanh(s(t)) * o(t)$ \n",
        "\n",
        "$x(t)$是LSTM输入向量，$h(t)$是LSTM是隐藏状态向量\n",
        "\n",
        "$i(t)$是输入/更新门的激活向量\n",
        "\n",
        "$o(t)$是输出门的激活向量\n",
        "\n",
        "$f(t)$是忘记门的激活向量\n",
        "\n",
        "$s(t)$细胞状态向量\n",
        "\n",
        "$f(t)$输入门的权值缩放向量\n",
        "\n",
        "$W_x \\in \\mathbb{R}^{h \\times d}, \\quad W_h \\in \\mathbb{R}^{h \\times h}\n",
        "，b \\in \\mathbb{R}^{h}$\n",
        "是我们训练期间要学习的权值和偏差，其中$h$和$d$分别为隐层数量和输入要素的数量\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mYqZeusmE1-l",
        "colab_type": "text"
      },
      "source": [
        "设损失函数为均方误差：\n",
        "$l(t)=f(h(t), y(t))=\\|h(t)-y(t)\\|^{2}$。其中, h(t)为隐层， y(t)为标签\n",
        "\n",
        "则目标函数为：$L=\\sum_{t=1}^{T} l(t)$\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJybER7_KPGQ",
        "colab_type": "text"
      },
      "source": [
        "**反向传播**\n",
        "\n",
        "$\\frac{d L}{d w}=\\sum_{t=1}^{T} \\sum_{i=1}^{M} \\frac{d L}{d h_{i}(t)} \\frac{d h_{i}(t)}{d w}$\n",
        "\n",
        "其中$h_{i}(t)$为第$i$个隐藏层，由于网实时更新，不断遗忘，很久之前的隐层并不会受到影响，所以有，\n",
        "\n",
        "$\\frac{d L}{d h_{i}(t)}=\\sum_{s=1}^{T} \\frac{d l(s)}{d h_{i}(t)}=\\sum_{s=t}^{T} \\frac{d l(s)}{d h_{i}(t)}$\n",
        "\n",
        "定义$L(t)$为第$t$步开始累计损失函数，所以有\n",
        "\n",
        "$\\frac{d L}{d h_{i}(t)}=\\sum_{s=t}^{T} \\frac{d l(s)}{d h_{i}(t)}=\\frac{d L(t)}{d h_{i}(t)}$\n",
        "\n",
        "反向传播可以写为，\n",
        "\n",
        "$\\frac{d L}{d w}=\\sum_{t=1}^{T} \\sum_{i=1}^{M} \\frac{d L(t)}{d h_{i}(t)} \\frac{d h_{i}(t)}{d w}$\n",
        "\n",
        "其中L(t)可以表示为\n",
        "\n",
        "$L(t)=\\left\\{\\begin{array}{ll}l(t)+L(t+1) & \\text { if } t<T \\\\ l(t) & \\text { if } t=T\\end{array}\\right.$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4sC5pP_SSEFV",
        "colab_type": "text"
      },
      "source": [
        "### GRU\n",
        "![替代文字](https://upload.wikimedia.org/wikipedia/commons/3/37/Gated_Recurrent_Unit%2C_base_type.svg?download)\n",
        "\n",
        "**数学表达**\n",
        "\n",
        "$\\boldsymbol{z}_{t}:=\\sigma\\left(\\boldsymbol{W}_{x z} \\boldsymbol{x}_{t}+\\boldsymbol{W}_{h z} \\boldsymbol{h}_{t-1}\\right)$\n",
        "\n",
        "$\\boldsymbol{r}_{t}:=\\sigma\\left(\\boldsymbol{W}_{x r} \\boldsymbol{x}_{t}+\\boldsymbol{W}_{h r} \\boldsymbol{h}_{t-1}\\right)$\n",
        "\n",
        "$\\tilde{\\boldsymbol{h}}_{t}:=\\phi\\left(\\boldsymbol{W}_{x h} \\boldsymbol{x}_{t}+\\boldsymbol{r}_{t} \\odot\\left(\\boldsymbol{W}_{h h} \\boldsymbol{h}_{t-1}\\right)\\right)$\n",
        "\n",
        "$\\boldsymbol{h}_{t}:=\\left(\\boldsymbol{1}-\\boldsymbol{z}_{t}\\right) \\odot \\tilde{\\boldsymbol{h}}_{t}+\\boldsymbol{z}_{t} \\odot \\boldsymbol{h}_{t-1}$\n",
        "\n",
        "$x_t,h_t$是输入向量和输出向量\n",
        "\n",
        "$z_t$是更新门矢量\n",
        "\n",
        "$r_t$是重置门矢量\n",
        "\n",
        "$\\tilde{\\boldsymbol{h}}_{t}$是缩放矢量"
      ]
    }
  ]
}